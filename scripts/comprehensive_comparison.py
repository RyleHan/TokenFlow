#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å…¨é¢å¯¹æ¯”æŠ¥å‘Šï¼šå¾®è°ƒå‰ vs å¾®è°ƒå(çº¯æ¨¡å‹) vs å¾®è°ƒåæ··åˆç­–ç•¥
æœ€å…¨é¢çš„æ€§èƒ½è¯„ä¼°å’Œåˆ†æ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from src.classifiers.hybrid_intent_classifier import HybridIntentClassifier
from src.classifiers.intent_classifier import MockIntentClassifier
import json
import time
from typing import Dict, List, Any
import numpy as np

class ComprehensiveComparator:
    """å…¨é¢å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ–å…¨é¢å¯¹æ¯”åˆ†æå™¨...")
        
        # è®¾ç½®è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ… ä½¿ç”¨MPSåŠ é€Ÿ")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ ä½¿ç”¨CPU")
        
        self.intent_categories = [
            "order_management", "user_auth", "payment", 
            "inventory", "notification", "none"
        ]
        
        # æ‰©å±•æµ‹è¯•æ ·æœ¬é›†ï¼ŒåŒ…å«å„ç§å¤æ‚åº¦
        self.test_cases = [
            # ç®€å•å•æ„å›¾
            {
                "prompt": "æˆ‘æƒ³æŸ¥çœ‹æˆ‘çš„è®¢å•çŠ¶æ€",
                "expected_intents": ["order_management"],
                "description": "ç®€å•å•æ„å›¾-è®¢å•æŸ¥è¯¢",
                "complexity": "simple"
            },
            {
                "prompt": "æ€ä¹ˆç»‘å®šç”¨æˆ·å",
                "expected_intents": ["user_auth"],
                "description": "ç®€å•å•æ„å›¾-ç”¨æˆ·è®¤è¯",
                "complexity": "simple"
            },
            {
                "prompt": "æˆ‘è¦ç»“ç®—è®¢å•",
                "expected_intents": ["order_management"],
                "description": "ç®€å•å•æ„å›¾-è®¢å•ç»“ç®—",
                "complexity": "simple"
            },
            
            # æ˜ç¡®å¤šæ„å›¾
            {
                "prompt": "ç™»å½•åæŸ¥çœ‹è®¢å•å¹¶å¤„ç†æ”¯ä»˜",
                "expected_intents": ["user_auth", "order_management", "payment"],
                "description": "æ˜ç¡®å¤šæ„å›¾-ç™»å½•+è®¢å•+æ”¯ä»˜",
                "complexity": "medium"
            },
            {
                "prompt": "æ”¯ä»˜æˆåŠŸåå‘é€é€šçŸ¥",
                "expected_intents": ["payment", "notification"],
                "description": "æ˜ç¡®å¤šæ„å›¾-æ”¯ä»˜+é€šçŸ¥",
                "complexity": "medium"
            },
            {
                "prompt": "åº“å­˜ä¸è¶³æ—¶éœ€è¦åŠæ—¶æé†’",
                "expected_intents": ["inventory", "notification"],
                "description": "æ˜ç¡®å¤šæ„å›¾-åº“å­˜+é€šçŸ¥",
                "complexity": "medium"
            },
            
            # å¤æ‚éšå«å¤šæ„å›¾
            {
                "prompt": "ä¿®æ”¹èº«ä»½è¦æ˜¯æœ‰é—®é¢˜å°±ä»˜æ¬¾å……å€¼",
                "expected_intents": ["user_auth", "payment"],
                "description": "å¤æ‚å¤šæ„å›¾-èº«ä»½ä¿®æ”¹+ä»˜æ¬¾",
                "complexity": "complex"
            },
            {
                "prompt": "å› ä¸ºè´¦å•é—®é¢˜éœ€è¦æ”¯ä»˜å› ä¸ºæ ‡è®°çŸ­ä¿¡",
                "expected_intents": ["payment", "notification"],
                "description": "å¤æ‚å¤šæ„å›¾-è´¦å•æ”¯ä»˜+çŸ­ä¿¡é€šçŸ¥",
                "complexity": "complex"
            },
            {
                "prompt": "ç”¨æˆ·æ³¨å†Œã€è®¢å•å¤„ç†å’Œæ”¯ä»˜ç¡®è®¤",
                "expected_intents": ["user_auth", "order_management", "payment"],
                "description": "å¤æ‚å¤šæ„å›¾-æ³¨å†Œ+è®¢å•+æ”¯ä»˜",
                "complexity": "complex"
            },
            
            # è¾¹ç•Œæƒ…å†µ
            {
                "prompt": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
                "expected_intents": ["none"],
                "description": "æ— å…³æŸ¥è¯¢-å¤©æ°”",
                "complexity": "simple"
            },
            {
                "prompt": "æ¨èä¸€éƒ¨å¥½çœ‹çš„ç”µå½±",
                "expected_intents": ["none"],
                "description": "æ— å…³æŸ¥è¯¢-ç”µå½±æ¨è",
                "complexity": "simple"
            },
            
            # æ­§ä¹‰å’Œå›°éš¾æ¡ˆä¾‹
            {
                "prompt": "è´¦æˆ·ç™»å½•è®¢å•æ”¯ä»˜åº“å­˜é€šçŸ¥å…¨éƒ¨éƒ½è¦å¤„ç†",
                "expected_intents": ["user_auth", "order_management", "payment", "inventory", "notification"],
                "description": "æå¤æ‚å¤šæ„å›¾-å…¨ä¸šåŠ¡è¦†ç›–",
                "complexity": "very_complex"
            }
        ]
    
    def load_pretrained_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½é¢„è®­ç»ƒQwen3-0.6Bæ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen3-0.6B",
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-0.6B",
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        model = model.to(self.device)
        model.eval()
        
        print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model
    
    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåæ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½å¾®è°ƒåQwen3-0.6Bæ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            "models/qwen3_fixed_classifier",
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-0.6B",
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        model = PeftModel.from_pretrained(base_model, "models/qwen3_fixed_classifier")
        model = model.to(self.device)
        model.eval()
        
        print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model
    
    def load_hybrid_classifier(self):
        """åŠ è½½æ··åˆåˆ†ç±»å™¨"""
        print("ğŸ“¥ åŠ è½½æ··åˆåˆ†ç±»å™¨...")
        
        try:
            hybrid_classifier = HybridIntentClassifier(use_finetuned=True)
            print("âœ… æ··åˆåˆ†ç±»å™¨åŠ è½½å®Œæˆ")
            return hybrid_classifier
        except Exception as e:
            print(f"âŒ æ··åˆåˆ†ç±»å™¨åŠ è½½å¤±è´¥: {e}")
            return None
    
    def predict_with_pretrained(self, model, tokenizer, prompt: str) -> List[str]:
        """é¢„è®­ç»ƒæ¨¡å‹é¢„æµ‹"""
        input_text = f"è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬çš„æ„å›¾ç±»åˆ«ï¼ˆorder_management, user_auth, payment, inventory, notification, noneï¼‰:\\n{prompt}\\næ„å›¾ï¼š"
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=256
        )
        
        if self.device.type != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if "æ„å›¾ï¼š" in response:
            predicted = response.split("æ„å›¾ï¼š")[-1].strip()
        elif "æ„å›¾:" in response:
            predicted = response.split("æ„å›¾:")[-1].strip()
        else:
            predicted = response.strip()
        
        return self._parse_intents(predicted)
    
    def predict_with_finetuned(self, model, tokenizer, prompt: str) -> List[str]:
        """å¾®è°ƒæ¨¡å‹é¢„æµ‹"""
        input_text = f"ç”¨æˆ·: {prompt}\\nåŠ©æ‰‹: "
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=256
        )
        
        if self.device.type != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=25,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if "åŠ©æ‰‹: " in response:
            predicted = response.split("åŠ©æ‰‹: ")[-1].strip()
        else:
            predicted = response.strip()
        
        return self._parse_intents(predicted)
    
    def predict_with_hybrid(self, hybrid_classifier, prompt: str) -> List[str]:
        """æ··åˆåˆ†ç±»å™¨é¢„æµ‹"""
        if hybrid_classifier is None:
            return ["none"]
        
        try:
            result = hybrid_classifier.classify(prompt)
            return result.get("intents", [result.get("intent", "none")])
        except Exception as e:
            print(f"âš ï¸ æ··åˆåˆ†ç±»å™¨é¢„æµ‹å¤±è´¥: {e}")
            return ["none"]
    
    def _parse_intents(self, predicted: str) -> List[str]:
        """è§£ææ„å›¾æ–‡æœ¬"""
        predicted = predicted.lower().replace("\\n", " ").replace("\\t", " ")
        found_intents = []
        
        for intent in self.intent_categories:
            if intent != "none":
                intent_clean = intent.replace("_", "")
                if intent in predicted or intent_clean in predicted.replace("_", ""):
                    found_intents.append(intent)
        
        if found_intents:
            return found_intents
        
        if "none" in predicted or not found_intents:
            return ["none"]
        
        return found_intents if found_intents else ["none"]
    
    def calculate_metrics(self, predicted: List[str], expected: List[str]) -> Dict[str, float]:
        """è®¡ç®—è¯¦ç»†æŒ‡æ ‡"""
        predicted_set = set(predicted)
        expected_set = set(expected)
        
        # ç²¾ç¡®åŒ¹é…
        exact_match = 1.0 if predicted_set == expected_set else 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(predicted_set & expected_set)
        union = len(predicted_set | expected_set)
        jaccard = intersection / union if union > 0 else 0.0
        
        # Precision, Recall, F1
        if len(predicted_set) == 0:
            precision = 0.0
        else:
            precision = intersection / len(predicted_set)
        
        if len(expected_set) == 0:
            recall = 0.0
        else:
            recall = intersection / len(expected_set)
        
        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * precision * recall / (precision + recall)
        
        return {
            "exact_match": exact_match,
            "jaccard": jaccard,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def estimate_token_usage(self, intents: List[str], prompt: str) -> Dict[str, int]:
        """ä¼°ç®—Tokenä½¿ç”¨é‡"""
        total_docs = 20
        avg_doc_tokens = 400
        
        if "none" in intents:
            retrieved_docs = 2
        else:
            base_docs = 3
            intent_bonus = len([i for i in intents if i != "none"])
            retrieved_docs = min(base_docs + intent_bonus, 8)
        
        without_filtering = total_docs * avg_doc_tokens
        with_filtering = retrieved_docs * avg_doc_tokens
        
        return {
            "without_filtering": without_filtering,
            "with_filtering": with_filtering,
            "tokens_saved": without_filtering - with_filtering,
            "savings_percentage": ((without_filtering - with_filtering) / without_filtering * 100) if without_filtering > 0 else 0
        }
    
    def run_comprehensive_comparison(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢å¯¹æ¯”"""
        print("ğŸš€ å¼€å§‹å…¨é¢å¯¹æ¯”åˆ†æ...")
        
        results = {
            "test_info": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_test_cases": len(self.test_cases),
                "comparison_types": ["é¢„è®­ç»ƒQwen3", "å¾®è°ƒçº¯æ¨¡å‹", "å¾®è°ƒæ··åˆç­–ç•¥"],
                "complexity_levels": ["simple", "medium", "complex", "very_complex"]
            },
            "individual_results": [],
            "summary_stats": {},
            "complexity_analysis": {},
            "token_usage_analysis": {}
        }
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        pretrained_tokenizer, pretrained_model = self.load_pretrained_model()
        finetuned_tokenizer, finetuned_model = self.load_finetuned_model()
        hybrid_classifier = self.load_hybrid_classifier()
        
        # ç´¯è®¡ç»Ÿè®¡
        stats = {
            "pretrained": {"exact_match": [], "jaccard": [], "f1": [], "precision": [], "recall": []},
            "finetuned": {"exact_match": [], "jaccard": [], "f1": [], "precision": [], "recall": []},
            "hybrid": {"exact_match": [], "jaccard": [], "f1": [], "precision": [], "recall": []}
        }
        
        # å¤æ‚åº¦ç»Ÿè®¡
        complexity_stats = {
            "simple": {"pretrained": [], "finetuned": [], "hybrid": []},
            "medium": {"pretrained": [], "finetuned": [], "hybrid": []},
            "complex": {"pretrained": [], "finetuned": [], "hybrid": []},
            "very_complex": {"pretrained": [], "finetuned": [], "hybrid": []}
        }
        
        # Tokenä½¿ç”¨ç»Ÿè®¡
        token_stats = {"pretrained": [], "finetuned": [], "hybrid": []}
        
        for i, test_case in enumerate(self.test_cases):
            print(f"\\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(self.test_cases)}: {test_case['description']}")
            print(f"   è¾“å…¥: {test_case['prompt']}")
            print(f"   æœŸæœ›: {test_case['expected_intents']}")
            print(f"   å¤æ‚åº¦: {test_case['complexity']}")
            
            test_result = {
                "test_case": test_case,
                "results": {}
            }
            
            # 1. é¢„è®­ç»ƒæ¨¡å‹
            try:
                pretrained_prediction = self.predict_with_pretrained(
                    pretrained_model, pretrained_tokenizer, test_case['prompt']
                )
                pretrained_metrics = self.calculate_metrics(pretrained_prediction, test_case['expected_intents'])
                pretrained_tokens = self.estimate_token_usage(pretrained_prediction, test_case['prompt'])
                
                test_result["results"]["pretrained"] = {
                    "predicted_intents": pretrained_prediction,
                    "metrics": pretrained_metrics,
                    "token_usage": pretrained_tokens
                }
                
                for key in stats["pretrained"]:
                    stats["pretrained"][key].append(pretrained_metrics[key])
                
                complexity_stats[test_case['complexity']]["pretrained"].append(pretrained_metrics["f1"])
                token_stats["pretrained"].append(pretrained_tokens["tokens_saved"])
                
                print(f"   é¢„è®­ç»ƒ:   {pretrained_prediction} (F1: {pretrained_metrics['f1']:.3f})")
                
            except Exception as e:
                print(f"   âŒ é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
                test_result["results"]["pretrained"] = {"error": str(e)}
            
            # 2. å¾®è°ƒçº¯æ¨¡å‹
            try:
                finetuned_prediction = self.predict_with_finetuned(
                    finetuned_model, finetuned_tokenizer, test_case['prompt']
                )
                finetuned_metrics = self.calculate_metrics(finetuned_prediction, test_case['expected_intents'])
                finetuned_tokens = self.estimate_token_usage(finetuned_prediction, test_case['prompt'])
                
                test_result["results"]["finetuned"] = {
                    "predicted_intents": finetuned_prediction,
                    "metrics": finetuned_metrics,
                    "token_usage": finetuned_tokens
                }
                
                for key in stats["finetuned"]:
                    stats["finetuned"][key].append(finetuned_metrics[key])
                
                complexity_stats[test_case['complexity']]["finetuned"].append(finetuned_metrics["f1"])
                token_stats["finetuned"].append(finetuned_tokens["tokens_saved"])
                
                print(f"   å¾®è°ƒçº¯æ¨¡å‹: {finetuned_prediction} (F1: {finetuned_metrics['f1']:.3f})")
                
            except Exception as e:
                print(f"   âŒ å¾®è°ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
                test_result["results"]["finetuned"] = {"error": str(e)}
            
            # 3. æ··åˆç­–ç•¥
            try:
                hybrid_prediction = self.predict_with_hybrid(hybrid_classifier, test_case['prompt'])
                hybrid_metrics = self.calculate_metrics(hybrid_prediction, test_case['expected_intents'])
                hybrid_tokens = self.estimate_token_usage(hybrid_prediction, test_case['prompt'])
                
                test_result["results"]["hybrid"] = {
                    "predicted_intents": hybrid_prediction,
                    "metrics": hybrid_metrics,
                    "token_usage": hybrid_tokens
                }
                
                for key in stats["hybrid"]:
                    stats["hybrid"][key].append(hybrid_metrics[key])
                
                complexity_stats[test_case['complexity']]["hybrid"].append(hybrid_metrics["f1"])
                token_stats["hybrid"].append(hybrid_tokens["tokens_saved"])
                
                print(f"   æ··åˆç­–ç•¥:   {hybrid_prediction} (F1: {hybrid_metrics['f1']:.3f})")
                
            except Exception as e:
                print(f"   âŒ æ··åˆç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
                test_result["results"]["hybrid"] = {"error": str(e)}
            
            results["individual_results"].append(test_result)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        def calc_avg(values):
            return sum(values) / len(values) if values else 0.0
        
        results["summary_stats"] = {}
        for model_type in ["pretrained", "finetuned", "hybrid"]:
            if stats[model_type]["f1"]:
                results["summary_stats"][model_type] = {
                    "avg_exact_match": calc_avg(stats[model_type]["exact_match"]),
                    "avg_jaccard": calc_avg(stats[model_type]["jaccard"]),
                    "avg_f1": calc_avg(stats[model_type]["f1"]),
                    "avg_precision": calc_avg(stats[model_type]["precision"]),
                    "avg_recall": calc_avg(stats[model_type]["recall"]),
                    "avg_token_savings": calc_avg(token_stats[model_type])
                }
        
        # å¤æ‚åº¦åˆ†æ
        results["complexity_analysis"] = {}
        for complexity in ["simple", "medium", "complex", "very_complex"]:
            results["complexity_analysis"][complexity] = {}
            for model_type in ["pretrained", "finetuned", "hybrid"]:
                if complexity_stats[complexity][model_type]:
                    results["complexity_analysis"][complexity][model_type] = calc_avg(complexity_stats[complexity][model_type])
        
        # Tokenä½¿ç”¨åˆ†æ
        results["token_usage_analysis"] = {
            "pretrained": calc_avg(token_stats["pretrained"]),
            "finetuned": calc_avg(token_stats["finetuned"]),
            "hybrid": calc_avg(token_stats["hybrid"])
        }
        
        return results
    
    def generate_comprehensive_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆå…¨é¢æŠ¥å‘Š"""
        print("\\n" + "="*100)
        print("ğŸ¯ TokenFlow æ¨¡å‹æ€§èƒ½å…¨é¢å¯¹æ¯”æŠ¥å‘Š")
        print("="*100)
        print(f"æµ‹è¯•æ—¶é—´: {results['test_info']['timestamp']}")
        print(f"æµ‹è¯•æ¡ˆä¾‹: {results['test_info']['total_test_cases']}ä¸ª")
        print(f"å¯¹æ¯”æ¨¡å‹: {', '.join(results['test_info']['comparison_types'])}")
        
        stats = results["summary_stats"]
        
        # æ€»ä½“æ€§èƒ½å¯¹æ¯”
        print("\\nğŸ“Š æ€»ä½“æ€§èƒ½å¯¹æ¯”")
        print("-"*100)
        print(f"{'æ¨¡å‹ç±»å‹':<20} {'ç²¾ç¡®åŒ¹é…':<10} {'Jaccard':<10} {'F1åˆ†æ•°':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'TokenèŠ‚çœ':<12}")
        print("-"*100)
        
        model_names = {
            "pretrained": "é¢„è®­ç»ƒQwen3-0.6B",
            "finetuned": "å¾®è°ƒçº¯æ¨¡å‹",
            "hybrid": "å¾®è°ƒæ··åˆç­–ç•¥"
        }
        
        for model_type in ["pretrained", "finetuned", "hybrid"]:
            if model_type in stats:
                s = stats[model_type]
                print(f"{model_names[model_type]:<20} {s['avg_exact_match']:<10.3f} {s['avg_jaccard']:<10.3f} {s['avg_f1']:<10.3f} {s['avg_precision']:<10.3f} {s['avg_recall']:<10.3f} {s['avg_token_savings']:<12.0f}")
        
        # æ”¹è¿›åˆ†æ
        if "pretrained" in stats and "finetuned" in stats and "hybrid" in stats:
            print("\\nğŸ’¡ æ”¹è¿›åˆ†æ")
            print("-"*100)
            
            # å¾®è°ƒçº¯æ¨¡å‹ vs é¢„è®­ç»ƒ
            pre_f1 = stats["pretrained"]["avg_f1"]
            fine_f1 = stats["finetuned"]["avg_f1"]
            hybrid_f1 = stats["hybrid"]["avg_f1"]
            
            print(f"ğŸš€ å¾®è°ƒçº¯æ¨¡å‹æ”¹è¿›:")
            print(f"   F1åˆ†æ•°: {pre_f1:.3f} â†’ {fine_f1:.3f} (+{((fine_f1-pre_f1)/pre_f1*100):+.1f}%)")
            print(f"   ç²¾ç¡®åŒ¹é…: {stats['pretrained']['avg_exact_match']:.3f} â†’ {stats['finetuned']['avg_exact_match']:.3f} (+{((stats['finetuned']['avg_exact_match']-stats['pretrained']['avg_exact_match'])/stats['pretrained']['avg_exact_match']*100):+.1f}%)")
            
            print(f"\\nğŸ­ æ··åˆç­–ç•¥æ•ˆæœ:")
            print(f"   vs é¢„è®­ç»ƒ: {pre_f1:.3f} â†’ {hybrid_f1:.3f} (+{((hybrid_f1-pre_f1)/pre_f1*100):+.1f}%)")
            print(f"   vs å¾®è°ƒçº¯æ¨¡å‹: {fine_f1:.3f} â†’ {hybrid_f1:.3f} ({((hybrid_f1-fine_f1)/fine_f1*100):+.1f}%)")
        
        # å¤æ‚åº¦åˆ†æ
        print("\\nğŸª ä¸åŒå¤æ‚åº¦åœºæ™¯è¡¨ç°")
        print("-"*100)
        print(f"{'å¤æ‚åº¦':<15} {'é¢„è®­ç»ƒ':<12} {'å¾®è°ƒçº¯æ¨¡å‹':<12} {'æ··åˆç­–ç•¥':<12} {'æœ€ä½³':<10}")
        print("-"*100)
        
        complexity_names = {
            "simple": "ç®€å•",
            "medium": "ä¸­ç­‰", 
            "complex": "å¤æ‚",
            "very_complex": "æå¤æ‚"
        }
        
        for complexity in ["simple", "medium", "complex", "very_complex"]:
            if complexity in results["complexity_analysis"]:
                comp_data = results["complexity_analysis"][complexity]
                
                pre_score = comp_data.get("pretrained", 0)
                fine_score = comp_data.get("finetuned", 0)
                hybrid_score = comp_data.get("hybrid", 0)
                
                best_score = max(pre_score, fine_score, hybrid_score)
                best_model = "é¢„è®­ç»ƒ" if best_score == pre_score else ("å¾®è°ƒçº¯æ¨¡å‹" if best_score == fine_score else "æ··åˆç­–ç•¥")
                
                print(f"{complexity_names[complexity]:<15} {pre_score:<12.3f} {fine_score:<12.3f} {hybrid_score:<12.3f} {best_model:<10}")
        
        # TokenèŠ‚çœåˆ†æ
        print("\\nğŸ’° TokenèŠ‚çœæ•ˆæœ")
        print("-"*60)
        token_usage = results["token_usage_analysis"]
        
        for model_type in ["pretrained", "finetuned", "hybrid"]:
            if model_type in token_usage:
                savings = token_usage[model_type]
                savings_pct = (savings / 8000) * 100  # ç›¸å¯¹äºæ— è¿‡æ»¤çš„8000 tokens
                print(f"{model_names[model_type]:<20}: å¹³å‡èŠ‚çœ {savings:.0f} tokens ({savings_pct:.1f}%)")
        
        # å…³é”®å‘ç°æ€»ç»“
        print("\\nğŸ¯ å…³é”®å‘ç°")
        print("-"*100)
        
        if "pretrained" in stats and "finetuned" in stats:
            improvement = ((stats["finetuned"]["avg_f1"] - stats["pretrained"]["avg_f1"]) / stats["pretrained"]["avg_f1"]) * 100
            
            if improvement > 100:
                print("ğŸŒŸ å¾®è°ƒå–å¾—å·¨å¤§æˆåŠŸï¼F1åˆ†æ•°æå‡è¶…è¿‡100%")
            elif improvement > 50:
                print("âœ¨ å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼Œæ€§èƒ½å¤§å¹…æå‡")
            elif improvement > 20:
                print("ğŸ“ˆ å¾®è°ƒæœ‰æ˜æ˜¾æ•ˆæœï¼Œæ€§èƒ½æ”¹å–„æ˜¾è‘—")
            
            # åˆ†ææ··åˆç­–ç•¥çš„ä»·å€¼
            if "hybrid" in stats:
                hybrid_vs_pure = ((stats["hybrid"]["avg_f1"] - stats["finetuned"]["avg_f1"]) / stats["finetuned"]["avg_f1"]) * 100
                
                if hybrid_vs_pure > 5:
                    print("ğŸ­ æ··åˆç­–ç•¥è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½")
                elif hybrid_vs_pure > -5:
                    print("ğŸ­ æ··åˆç­–ç•¥ä¿æŒäº†ç¨³å®šçš„æ€§èƒ½")
                else:
                    print("ğŸ­ æ··åˆç­–ç•¥ç•¥å¾®é™ä½äº†çº¯æ¨¡å‹æ€§èƒ½")
        
        # æœ€ä½³é…ç½®æ¨è
        print("\\nğŸ† æœ€ä½³é…ç½®æ¨è")
        print("-"*60)
        
        if "hybrid" in stats and "finetuned" in stats:
            if stats["hybrid"]["avg_f1"] >= stats["finetuned"]["avg_f1"]:
                print("ğŸ¯ æ¨èä½¿ç”¨ï¼šå¾®è°ƒæ··åˆç­–ç•¥")
                print("   ç†ç”±ï¼šåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æä¾›æ›´ç¨³å®šçš„é¢„æµ‹")
            else:
                print("ğŸ¯ æ¨èä½¿ç”¨ï¼šå¾®è°ƒçº¯æ¨¡å‹") 
                print("   ç†ç”±ï¼šçº¯ç¥ç»ç½‘ç»œæ¨¡å‹æ€§èƒ½æœ€ä¼˜")
        
        print("\\nğŸŠ è¯„ä¼°å®Œæˆï¼æ‰€æœ‰æ¨¡å‹é…ç½®å·²å…¨é¢å¯¹æ¯”åˆ†æã€‚")

def main():
    """ä¸»å‡½æ•°"""
    comparator = ComprehensiveComparator()
    
    # è¿è¡Œå…¨é¢å¯¹æ¯”
    results = comparator.run_comprehensive_comparison()
    
    # ç”ŸæˆæŠ¥å‘Š
    comparator.generate_comprehensive_report(results)
    
    # ä¿å­˜ç»“æœ
    try:
        with open("reports/comprehensive_comparison.json", "w", encoding="utf-8") as f:
            # å¤„ç†numpyç±»å‹
            results_clean = json.loads(json.dumps(results, default=str))
            json.dump(results_clean, f, ensure_ascii=False, indent=2)
        
        print(f"\\nâœ… è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: reports/comprehensive_comparison.json")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main()