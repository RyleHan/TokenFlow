#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çº¯æ¨¡å‹å¯¹æ¯”ï¼šå¾®è°ƒå‰åQwen3-0.6Bæ¨¡å‹æ€§èƒ½å¯¹æ¯”
ä¸ä½¿ç”¨æ··åˆç­–ç•¥ï¼Œåªæµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹æœ¬èº«
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
import time
from typing import Dict, List, Any

class PureModelComparator:
    """çº¯ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹æ¯”å™¨"""
    
    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ–çº¯æ¨¡å‹å¯¹æ¯”å™¨...")
        
        # è®¾ç½®è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ… ä½¿ç”¨MPSåŠ é€Ÿ")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ ä½¿ç”¨CPU")
        
        self.intent_categories = [
            "order_management", "user_auth", "payment", 
            "inventory", "notification", "none"
        ]
        
        # æµ‹è¯•æ ·æœ¬
        self.test_cases = [
            {
                "prompt": "æˆ‘æƒ³æŸ¥çœ‹æˆ‘çš„è®¢å•çŠ¶æ€",
                "expected_intents": ["order_management"],
                "description": "å•æ„å›¾-è®¢å•æŸ¥è¯¢"
            },
            {
                "prompt": "ç™»å½•åæŸ¥çœ‹è®¢å•å¹¶å¤„ç†æ”¯ä»˜",
                "expected_intents": ["user_auth", "order_management", "payment"],
                "description": "å¤šæ„å›¾-ç”¨æˆ·è®¤è¯+è®¢å•+æ”¯ä»˜"
            },
            {
                "prompt": "æ”¯ä»˜æˆåŠŸåå‘é€é€šçŸ¥",
                "expected_intents": ["payment", "notification"],
                "description": "å¤šæ„å›¾-æ”¯ä»˜+é€šçŸ¥"
            },
            {
                "prompt": "åº“å­˜ä¸è¶³æ—¶éœ€è¦åŠæ—¶æé†’",
                "expected_intents": ["inventory", "notification"],
                "description": "å¤šæ„å›¾-åº“å­˜+é€šçŸ¥"
            },
            {
                "prompt": "ç”¨æˆ·æ³¨å†Œã€è®¢å•å¤„ç†å’Œæ”¯ä»˜ç¡®è®¤",
                "expected_intents": ["user_auth", "order_management", "payment"],
                "description": "å¤æ‚å¤šæ„å›¾"
            },
            {
                "prompt": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
                "expected_intents": ["none"],
                "description": "æ— å…³æŸ¥è¯¢"
            },
            {
                "prompt": "ä¿®æ”¹èº«ä»½è¦æ˜¯æœ‰é—®é¢˜å°±ä»˜æ¬¾å……å€¼",
                "expected_intents": ["user_auth", "payment"],
                "description": "å¤æ‚è¡¨è¾¾-èº«ä»½+æ”¯ä»˜"
            },
            {
                "prompt": "å› ä¸ºè´¦å•é—®é¢˜éœ€è¦æ”¯ä»˜å› ä¸ºæ ‡è®°çŸ­ä¿¡",
                "expected_intents": ["payment", "notification"],
                "description": "å¤æ‚è¡¨è¾¾-æ”¯ä»˜+é€šçŸ¥"
            },
            {
                "prompt": "æ€ä¹ˆç»‘å®šç”¨æˆ·å",
                "expected_intents": ["user_auth"],
                "description": "å•æ„å›¾-ç”¨æˆ·è®¤è¯"
            },
            {
                "prompt": "æˆ‘è¦ç»“ç®—è®¢å•",
                "expected_intents": ["order_management"],
                "description": "å•æ„å›¾-è®¢å•ç»“ç®—"
            }
        ]
    
    def load_pretrained_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½é¢„è®­ç»ƒQwen3-0.6Bæ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            "Qwen/Qwen3-0.6B",
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-0.6B",
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        model = model.to(self.device)
        model.eval()
        
        print("âœ… é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model
    
    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåæ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½å¾®è°ƒåQwen3-0.6Bæ¨¡å‹...")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            "models/qwen3_fixed_classifier",
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-0.6B",
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        # åŠ è½½å¾®è°ƒé€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, "models/qwen3_fixed_classifier")
        model = model.to(self.device)
        model.eval()
        
        print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model
    
    def predict_with_model(self, model, tokenizer, prompt: str, is_finetuned: bool = False) -> List[str]:
        """ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        
        if is_finetuned:
            # å¾®è°ƒæ¨¡å‹ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ¼å¼
            input_text = f"ç”¨æˆ·: {prompt}\\nåŠ©æ‰‹: "
        else:
            # é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨é€šç”¨æ ¼å¼
            input_text = f"è¯·è¯†åˆ«ä»¥ä¸‹æ–‡æœ¬çš„æ„å›¾ç±»åˆ«ï¼ˆorder_management, user_auth, payment, inventory, notification, noneï¼‰:\\n{prompt}\\næ„å›¾ï¼š"
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=256
        )
        
        if self.device.type != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        if is_finetuned:
            # å¾®è°ƒæ¨¡å‹è§£æ
            if "åŠ©æ‰‹: " in response:
                predicted = response.split("åŠ©æ‰‹: ")[-1].strip()
            else:
                predicted = response.strip()
        else:
            # é¢„è®­ç»ƒæ¨¡å‹è§£æ
            if "æ„å›¾ï¼š" in response:
                predicted = response.split("æ„å›¾ï¼š")[-1].strip()
            elif "æ„å›¾:" in response:
                predicted = response.split("æ„å›¾:")[-1].strip()
            else:
                predicted = response.strip()
        
        # è§£ææ„å›¾
        predicted = predicted.lower().replace("\\n", " ").replace("\\t", " ")
        found_intents = []
        
        for intent in self.intent_categories:
            if intent != "none":
                intent_clean = intent.replace("_", "")
                if intent in predicted or intent_clean in predicted.replace("_", ""):
                    found_intents.append(intent)
        
        # åå¤„ç†ï¼šå¦‚æœæ‰¾åˆ°ä¸šåŠ¡æ„å›¾ï¼Œå°±ä¸è¿”å›none
        if found_intents:
            return found_intents
        
        # æ£€æŸ¥æ˜¯å¦æ˜ç¡®ä¸ºnone
        if "none" in predicted or not found_intents:
            return ["none"]
        
        return found_intents if found_intents else ["none"]
    
    def calculate_metrics(self, predicted: List[str], expected: List[str]) -> Dict[str, float]:
        """è®¡ç®—è¯¦ç»†æŒ‡æ ‡"""
        predicted_set = set(predicted)
        expected_set = set(expected)
        
        # ç²¾ç¡®åŒ¹é…
        exact_match = 1.0 if predicted_set == expected_set else 0.0
        
        # Jaccardç›¸ä¼¼åº¦
        intersection = len(predicted_set & expected_set)
        union = len(predicted_set | expected_set)
        jaccard = intersection / union if union > 0 else 0.0
        
        # Precision, Recall, F1
        if len(predicted_set) == 0:
            precision = 0.0
        else:
            precision = intersection / len(predicted_set)
        
        if len(expected_set) == 0:
            recall = 0.0
        else:
            recall = intersection / len(expected_set)
        
        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * precision * recall / (precision + recall)
        
        return {
            "exact_match": exact_match,
            "jaccard": jaccard,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def run_comparison(self) -> Dict[str, Any]:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹çº¯æ¨¡å‹å¯¹æ¯”æµ‹è¯•...")
        
        results = {
            "test_info": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_test_cases": len(self.test_cases),
                "model_comparison": "é¢„è®­ç»ƒ vs å¾®è°ƒ Qwen3-0.6B"
            },
            "individual_results": [],
            "summary_stats": {}
        }
        
        # åŠ è½½æ¨¡å‹
        pretrained_tokenizer, pretrained_model = self.load_pretrained_model()
        finetuned_tokenizer, finetuned_model = self.load_finetuned_model()
        
        # ç´¯è®¡ç»Ÿè®¡
        pretrained_stats = {"exact_match": [], "jaccard": [], "f1": [], "precision": [], "recall": []}
        finetuned_stats = {"exact_match": [], "jaccard": [], "f1": [], "precision": [], "recall": []}
        
        for i, test_case in enumerate(self.test_cases):
            print(f"\\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(self.test_cases)}: {test_case['description']}")
            print(f"   è¾“å…¥: {test_case['prompt']}")
            print(f"   æœŸæœ›: {test_case['expected_intents']}")
            
            test_result = {
                "test_case": test_case,
                "results": {}
            }
            
            # 1. é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•
            try:
                pretrained_prediction = self.predict_with_model(
                    pretrained_model, pretrained_tokenizer, test_case['prompt'], is_finetuned=False
                )
                pretrained_metrics = self.calculate_metrics(pretrained_prediction, test_case['expected_intents'])
                
                test_result["results"]["pretrained"] = {
                    "predicted_intents": pretrained_prediction,
                    "metrics": pretrained_metrics
                }
                
                # ç´¯è®¡ç»Ÿè®¡
                for key in pretrained_stats:
                    pretrained_stats[key].append(pretrained_metrics[key])
                
                print(f"   é¢„è®­ç»ƒ:   {pretrained_prediction} (F1: {pretrained_metrics['f1']:.3f})")
                
            except Exception as e:
                print(f"   âŒ é¢„è®­ç»ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
                test_result["results"]["pretrained"] = {"error": str(e)}
            
            # 2. å¾®è°ƒæ¨¡å‹æµ‹è¯•  
            try:
                finetuned_prediction = self.predict_with_model(
                    finetuned_model, finetuned_tokenizer, test_case['prompt'], is_finetuned=True
                )
                finetuned_metrics = self.calculate_metrics(finetuned_prediction, test_case['expected_intents'])
                
                test_result["results"]["finetuned"] = {
                    "predicted_intents": finetuned_prediction,
                    "metrics": finetuned_metrics
                }
                
                # ç´¯è®¡ç»Ÿè®¡
                for key in finetuned_stats:
                    finetuned_stats[key].append(finetuned_metrics[key])
                
                print(f"   å¾®è°ƒå:   {finetuned_prediction} (F1: {finetuned_metrics['f1']:.3f})")
                
            except Exception as e:
                print(f"   âŒ å¾®è°ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
                test_result["results"]["finetuned"] = {"error": str(e)}
            
            results["individual_results"].append(test_result)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        def calc_avg(values):
            return sum(values) / len(values) if values else 0.0
        
        results["summary_stats"] = {
            "pretrained": {
                "avg_exact_match": calc_avg(pretrained_stats["exact_match"]),
                "avg_jaccard": calc_avg(pretrained_stats["jaccard"]),
                "avg_f1": calc_avg(pretrained_stats["f1"]),
                "avg_precision": calc_avg(pretrained_stats["precision"]),
                "avg_recall": calc_avg(pretrained_stats["recall"]),
            },
            "finetuned": {
                "avg_exact_match": calc_avg(finetuned_stats["exact_match"]),
                "avg_jaccard": calc_avg(finetuned_stats["jaccard"]),
                "avg_f1": calc_avg(finetuned_stats["f1"]),
                "avg_precision": calc_avg(finetuned_stats["precision"]),
                "avg_recall": calc_avg(finetuned_stats["recall"]),
            }
        }
        
        # è®¡ç®—æ”¹è¿›
        if finetuned_stats["f1"] and pretrained_stats["f1"]:
            pretrained_f1 = results["summary_stats"]["pretrained"]["avg_f1"]
            finetuned_f1 = results["summary_stats"]["finetuned"]["avg_f1"]
            
            results["summary_stats"]["improvement"] = {
                "f1_absolute": finetuned_f1 - pretrained_f1,
                "f1_relative": ((finetuned_f1 - pretrained_f1) / pretrained_f1 * 100) if pretrained_f1 > 0 else 0,
                "exact_match_improvement": results["summary_stats"]["finetuned"]["avg_exact_match"] - results["summary_stats"]["pretrained"]["avg_exact_match"]
            }
        
        return results
    
    def generate_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\\n" + "="*80)
        print("ğŸ¯ çº¯ç¥ç»ç½‘ç»œæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š (å¾®è°ƒå‰ vs å¾®è°ƒå)")
        print("="*80)
        
        stats = results["summary_stats"]
        
        if "pretrained" in stats and "finetuned" in stats:
            print("\\nğŸ“Š è¯¦ç»†æ€§èƒ½å¯¹æ¯”:")
            print("-"*80)
            print(f"{'æŒ‡æ ‡':<15} {'é¢„è®­ç»ƒæ¨¡å‹':<15} {'å¾®è°ƒåæ¨¡å‹':<15} {'æ”¹è¿›':<15}")
            print("-"*80)
            
            metrics = ["avg_exact_match", "avg_jaccard", "avg_f1", "avg_precision", "avg_recall"]
            metric_names = ["ç²¾ç¡®åŒ¹é…", "Jaccard", "F1åˆ†æ•°", "ç²¾ç¡®ç‡", "å¬å›ç‡"]
            
            for metric, name in zip(metrics, metric_names):
                pretrained_val = stats["pretrained"][metric]
                finetuned_val = stats["finetuned"][metric]
                improvement = finetuned_val - pretrained_val
                
                print(f"{name:<15} {pretrained_val:<15.3f} {finetuned_val:<15.3f} {improvement:+.3f}")
            
            if "improvement" in stats:
                imp = stats["improvement"]
                print("\\nğŸ’¡ å…³é”®æ”¹è¿›åˆ†æ:")
                print(f"  ğŸ¯ F1åˆ†æ•°æå‡: {imp['f1_absolute']:+.3f} ({imp['f1_relative']:+.1f}%)")
                print(f"  ğŸ¯ ç²¾ç¡®åŒ¹é…æå‡: {imp['exact_match_improvement']:+.3f}")
                
                print("\\nğŸš€ å¾®è°ƒæ•ˆæœè¯„ä¼°:")
                if imp['f1_relative'] > 100:
                    print("  ğŸŒŸ å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼F1åˆ†æ•°æå‡è¶…è¿‡100%")
                elif imp['f1_relative'] > 50:
                    print("  âœ¨ å¾®è°ƒæ•ˆæœè‰¯å¥½ï¼ŒF1åˆ†æ•°æœ‰å¤§å¹…æå‡")
                elif imp['f1_relative'] > 20:
                    print("  ğŸ“ˆ å¾®è°ƒæœ‰æ˜æ˜¾æ•ˆæœï¼Œæ€§èƒ½æœ‰æ‰€æ”¹å–„")
                elif imp['f1_relative'] > 0:
                    print("  ğŸ“Š å¾®è°ƒæœ‰ä¸€å®šæ•ˆæœï¼Œç•¥æœ‰æ”¹è¿›")
                else:
                    print("  âš ï¸ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        # åˆ†æå…·ä½“æ¡ˆä¾‹
        print("\\nğŸ” å…¸å‹æ¡ˆä¾‹åˆ†æ:")
        for result in results["individual_results"]:
            if "pretrained" in result["results"] and "finetuned" in result["results"]:
                if "error" not in result["results"]["pretrained"] and "error" not in result["results"]["finetuned"]:
                    case = result["test_case"]
                    pre_f1 = result["results"]["pretrained"]["metrics"]["f1"]
                    fine_f1 = result["results"]["finetuned"]["metrics"]["f1"]
                    
                    if fine_f1 - pre_f1 > 0.5:  # æ˜¾è‘—æ”¹è¿›çš„æ¡ˆä¾‹
                        print(f"  âœ¨ {case['description']}: F1ä»{pre_f1:.3f}æå‡åˆ°{fine_f1:.3f}")
                        print(f"     è¾“å…¥: {case['prompt']}")
                        print(f"     é¢„è®­ç»ƒ: {result['results']['pretrained']['predicted_intents']}")
                        print(f"     å¾®è°ƒå: {result['results']['finetuned']['predicted_intents']}")
                        print(f"     æœŸæœ›: {case['expected_intents']}")

def main():
    """ä¸»å‡½æ•°"""
    comparator = PureModelComparator()
    
    # è¿è¡Œå¯¹æ¯”
    results = comparator.run_comparison()
    
    # ç”ŸæˆæŠ¥å‘Š
    comparator.generate_report(results)
    
    # ä¿å­˜ç»“æœ
    with open("reports/pure_model_comparison.json", "w", encoding="utf-8") as f:
        # å¤„ç†å¯èƒ½çš„numpyç±»å‹
        def convert_types(obj):
            if hasattr(obj, 'item'):
                return obj.item()
            return obj
        
        import numpy as np
        results_clean = json.loads(json.dumps(results, default=str))
        json.dump(results_clean, f, ensure_ascii=False, indent=2)
    
    print(f"\\nâœ… è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: reports/pure_model_comparison.json")

if __name__ == "__main__":
    main()