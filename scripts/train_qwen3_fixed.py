#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¿®å¤ç‰ˆQwen3-0.6Bå¾®è°ƒè®­ç»ƒè„šæœ¬
è§£å†³tokenizationé—®é¢˜
"""

import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig, 
    get_peft_model, 
    TaskType,
)
from datasets import Dataset
import os
import psutil

def load_and_prepare_data(data_path):
    """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
    print(f"æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
    
    conversations = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            sample = json.loads(line.strip())
            prompt = sample['prompt']
            intents = sample['output']
            
            # æ ¼å¼åŒ–ä¸ºå¯¹è¯
            intent_text = ", ".join(intents)
            conversation = f"ç”¨æˆ·: {prompt}\\nåŠ©æ‰‹: {intent_text}"
            conversations.append(conversation)
    
    print(f"æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…± {len(conversations)} ä¸ªæ ·æœ¬")
    return conversations

def tokenize_data(texts, tokenizer):
    """ç®€å•çš„tokenization"""
    print("æ­£åœ¨tokenizeæ•°æ®...")
    
    tokenized = []
    for text in texts:
        tokens = tokenizer(
            text,
            truncation=True,
            padding=False,
            max_length=256,
            return_tensors=None,
        )
        # æ·»åŠ labels
        tokens["labels"] = tokens["input_ids"].copy()
        tokenized.append(tokens)
    
    return tokenized

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¿®å¤ç‰ˆQwen3-0.6Bæ„å›¾åˆ†ç±»å™¨è®­ç»ƒ")
    print("=" * 50)
    
    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… ä½¿ç”¨Apple Metal Performance Shaders (MPS)åŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("âš ï¸ ä½¿ç”¨CPU")
    
    # æ£€æŸ¥å†…å­˜
    memory = psutil.virtual_memory()
    print(f"ç³»ç»Ÿå†…å­˜: {memory.total // (1024**3)}GB")
    print(f"å¯ç”¨å†…å­˜: {memory.available // (1024**3)}GB")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_name = "Qwen/Qwen3-0.6B"
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="right"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    )
    
    model = model.to(device)
    print("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆ")
    
    # è®¾ç½®LoRA
    print("æ­£åœ¨é…ç½®LoRA...")
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # å‡†å¤‡æ•°æ®
    data_path = "data/enhanced_multi_intent_training_data.jsonl"
    conversations = load_and_prepare_data(data_path)
    
    # Tokenizeæ•°æ®
    tokenized_data = tokenize_data(conversations, tokenizer)
    
    # åˆ›å»ºDataset
    dataset = Dataset.from_list(tokenized_data)
    print(f"åˆ›å»ºæ•°æ®é›†å®Œæˆï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # è®­ç»ƒå‚æ•°
    output_dir = "models/qwen3_fixed_classifier"
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        num_train_epochs=2,
        learning_rate=2e-4,
        fp16=False,
        logging_steps=50,
        save_steps=500,
        save_total_limit=2,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
        report_to=None,
        warmup_steps=20,
        max_grad_norm=1.0,
        save_safetensors=True,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆ")
        
        # æ¸…ç†ç¼“å­˜
        if device.type == "mps":
            torch.mps.empty_cache()
            print("ğŸ§¹ MPSç¼“å­˜å·²æ¸…ç†")
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        # å¿«é€Ÿæµ‹è¯•
        print("\\nğŸ§ª å¿«é€Ÿæµ‹è¯•...")
        test_prompts = [
            "æˆ‘æƒ³æŸ¥çœ‹è®¢å•çŠ¶æ€",
            "ç™»å½•åæŸ¥çœ‹è®¢å•å¹¶å¤„ç†æ”¯ä»˜", 
            "æ”¯ä»˜æˆåŠŸåå‘é€é€šçŸ¥",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"
        ]
        
        for prompt in test_prompts:
            input_text = f"ç”¨æˆ·: {prompt}\\nåŠ©æ‰‹: "
            inputs = tokenizer(input_text, return_tensors="pt", max_length=256, truncation=True)
            
            if device.type != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=20,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            predicted = response.split("åŠ©æ‰‹: ")[-1].strip()
            
            print(f"è¾“å…¥: {prompt}")
            print(f"é¢„æµ‹: {predicted}")
            print("-" * 40)
        
        print("ğŸ‰ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()