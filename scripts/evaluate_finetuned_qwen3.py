#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¯„ä¼°å¾®è°ƒåçš„Qwen3-0.6Bæ¨¡å‹æ€§èƒ½
å¯¹æ¯”å¾®è°ƒå‰åçš„æ•ˆæœ
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os
import time
from typing import List, Dict, Tuple
import random
from collections import defaultdict

class Qwen3Evaluator:
    """Qwen3æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.base_model_name = "Qwen/Qwen3-0.6B"
        self.finetuned_model_path = "models/qwen3_fixed_classifier"
        self.intent_categories = [
            "order_management", "user_auth", "payment", 
            "inventory", "notification", "none"
        ]
        
        # æ£€æŸ¥è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("âœ… ä½¿ç”¨Apple Metal Performance Shaders (MPS)")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ ä½¿ç”¨CPU")
    
    def load_test_data(self, test_size: int = 50) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ® (å‰{test_size}ä¸ªæ ·æœ¬)...")
        
        test_data = []
        data_path = "data/enhanced_multi_intent_training_data.jsonl"
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= test_size:
                    break
                    
                sample = json.loads(line.strip())
                test_data.append({
                    "prompt": sample["prompt"],
                    "expected": sample["output"]
                })
        
        print(f"æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆ: {len(test_data)} ä¸ªæ ·æœ¬")
        return test_data
    
    def load_pretrained_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        model = model.to(self.device)
        print("é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        
        return tokenizer, model
    
    def load_finetuned_model(self):
        """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½å¾®è°ƒåçš„æ¨¡å‹...")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            self.finetuned_model_path,
            trust_remote_code=True,
            padding_side="right"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        # åŠ è½½å¾®è°ƒé€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, self.finetuned_model_path)
        model = model.to(self.device)
        
        print("å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model
    
    def predict_intent(self, model, tokenizer, prompt: str) -> List[str]:
        """é¢„æµ‹æ„å›¾"""
        input_text = f"ç”¨æˆ·: {prompt}\\nåŠ©æ‰‹: "
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=256
        )
        
        if self.device.type != "cpu":
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=25,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        predicted = response.split("åŠ©æ‰‹: ")[-1].strip()
        
        # è§£æé¢„æµ‹çš„æ„å›¾
        predicted_intents = []
        for intent in self.intent_categories:
            if intent in predicted.lower():
                predicted_intents.append(intent)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ„å›¾ï¼Œè¿”å›none
        if not predicted_intents:
            predicted_intents = ["none"]
        
        return predicted_intents
    
    def calculate_accuracy(self, predictions: List[List[str]], targets: List[List[str]]) -> Dict:
        """è®¡ç®—å‡†ç¡®ç‡"""
        total = len(predictions)
        exact_match = 0
        partial_match = 0
        
        for pred, target in zip(predictions, targets):
            pred_set = set(pred)
            target_set = set(target)
            
            if pred_set == target_set:
                exact_match += 1
                partial_match += 1
            elif pred_set & target_set:  # æœ‰äº¤é›†
                partial_match += 1
        
        return {
            "exact_match_accuracy": exact_match / total,
            "partial_match_accuracy": partial_match / total,
            "total_samples": total
        }
    
    def evaluate_model(self, model, tokenizer, test_data: List[Dict], model_name: str) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        print(f"\\næ­£åœ¨è¯„ä¼° {model_name}...")
        
        predictions = []
        targets = []
        
        start_time = time.time()
        
        for i, sample in enumerate(test_data):
            if i % 20 == 0:
                print(f"  è¿›åº¦: {i}/{len(test_data)}")
            
            pred = self.predict_intent(model, tokenizer, sample["prompt"])
            predictions.append(pred)
            targets.append(sample["expected"])
        
        eval_time = time.time() - start_time
        
        # è®¡ç®—å‡†ç¡®ç‡
        metrics = self.calculate_accuracy(predictions, targets)
        metrics["evaluation_time"] = eval_time
        metrics["model_name"] = model_name
        
        print(f"{model_name} è¯„ä¼°å®Œæˆ:")
        print(f"  ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡: {metrics['exact_match_accuracy']:.3f}")
        print(f"  éƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡: {metrics['partial_match_accuracy']:.3f}")
        print(f"  è¯„ä¼°æ—¶é—´: {eval_time:.2f}ç§’")
        
        return metrics, predictions, targets
    
    def analyze_predictions(self, predictions: List[List[str]], targets: List[List[str]], 
                          test_data: List[Dict], model_name: str):
        """åˆ†æé¢„æµ‹ç»“æœ"""
        print(f"\\nğŸ“Š {model_name} è¯¦ç»†åˆ†æ:")
        
        # æŒ‰æ„å›¾ç±»åˆ«åˆ†æ
        intent_stats = defaultdict(lambda: {"correct": 0, "total": 0})
        
        for pred, target, sample in zip(predictions, targets, test_data):
            for intent in target:
                intent_stats[intent]["total"] += 1
                if intent in pred:
                    intent_stats[intent]["correct"] += 1
        
        print("\\nå„æ„å›¾ç±»åˆ«å‡†ç¡®ç‡:")
        for intent, stats in intent_stats.items():
            if stats["total"] > 0:
                accuracy = stats["correct"] / stats["total"]
                print(f"  {intent}: {accuracy:.3f} ({stats['correct']}/{stats['total']})")
        
        # æ˜¾ç¤ºé”™è¯¯æ¡ˆä¾‹
        print(f"\\nâŒ {model_name} é”™è¯¯é¢„æµ‹ç¤ºä¾‹:")
        error_count = 0
        for pred, target, sample in zip(predictions, targets, test_data):
            if set(pred) != set(target) and error_count < 5:
                print(f"  è¾“å…¥: {sample['prompt']}")
                print(f"  æœŸæœ›: {target}")
                print(f"  é¢„æµ‹: {pred}")
                print(f"  ---")
                error_count += 1
    
    def run_comparison(self):
        """è¿è¡Œå¯¹æ¯”è¯„ä¼°"""
        print("ğŸ¯ Qwen3-0.6B å¾®è°ƒå‰åæ€§èƒ½å¯¹æ¯”è¯„ä¼°")
        print("=" * 60)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.load_test_data(50)
        
        results = {}
        
        try:
            # è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹
            print("\\nğŸ” è¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹...")
            pretrained_tokenizer, pretrained_model = self.load_pretrained_model()
            pretrained_metrics, pretrained_preds, targets = self.evaluate_model(
                pretrained_model, pretrained_tokenizer, test_data, "é¢„è®­ç»ƒQwen3-0.6B"
            )
            results["pretrained"] = pretrained_metrics
            
            # åˆ†æé¢„è®­ç»ƒæ¨¡å‹ç»“æœ
            self.analyze_predictions(pretrained_preds, targets, test_data, "é¢„è®­ç»ƒæ¨¡å‹")
            
            # æ¸…ç†å†…å­˜
            del pretrained_model, pretrained_tokenizer
            if self.device.type == "mps":
                torch.mps.empty_cache()
            
        except Exception as e:
            print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            results["pretrained"] = {"error": str(e)}
        
        try:
            # è¯„ä¼°å¾®è°ƒæ¨¡å‹
            print("\\nğŸ” è¯„ä¼°å¾®è°ƒåæ¨¡å‹...")
            finetuned_tokenizer, finetuned_model = self.load_finetuned_model()
            finetuned_metrics, finetuned_preds, targets = self.evaluate_model(
                finetuned_model, finetuned_tokenizer, test_data, "å¾®è°ƒåQwen3-0.6B"
            )
            results["finetuned"] = finetuned_metrics
            
            # åˆ†æå¾®è°ƒæ¨¡å‹ç»“æœ
            self.analyze_predictions(finetuned_preds, targets, test_data, "å¾®è°ƒæ¨¡å‹")
            
        except Exception as e:
            print(f"âŒ å¾®è°ƒæ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            results["finetuned"] = {"error": str(e)}
        
        # å¯¹æ¯”åˆ†æ
        self.generate_comparison_report(results)
        
        # ä¿å­˜ç»“æœ
        with open("reports/finetuning_comparison_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print("\\nâœ… è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: reports/finetuning_comparison_results.json")
        return results
    
    def generate_comparison_report(self, results: Dict):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\\nğŸ“ˆ å¾®è°ƒæ•ˆæœå¯¹æ¯”æŠ¥å‘Š")
        print("=" * 60)
        
        if "pretrained" in results and "finetuned" in results:
            if "error" not in results["pretrained"] and "error" not in results["finetuned"]:
                pretrained = results["pretrained"]
                finetuned = results["finetuned"]
                
                print("\\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”:")
                print(f"{'æŒ‡æ ‡':<20} {'é¢„è®­ç»ƒ':<15} {'å¾®è°ƒå':<15} {'æå‡':<15}")
                print("-" * 70)
                
                # ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡
                pre_exact = pretrained["exact_match_accuracy"]
                fine_exact = finetuned["exact_match_accuracy"]
                exact_improvement = fine_exact - pre_exact
                exact_relative = (exact_improvement / pre_exact * 100) if pre_exact > 0 else 0
                
                print(f"{'ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡':<20} {pre_exact:<15.3f} {fine_exact:<15.3f} {exact_improvement:+.3f} ({exact_relative:+.1f}%)")
                
                # éƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡
                pre_partial = pretrained["partial_match_accuracy"]
                fine_partial = finetuned["partial_match_accuracy"]
                partial_improvement = fine_partial - pre_partial
                partial_relative = (partial_improvement / pre_partial * 100) if pre_partial > 0 else 0
                
                print(f"{'éƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡':<20} {pre_partial:<15.3f} {fine_partial:<15.3f} {partial_improvement:+.3f} ({partial_relative:+.1f}%)")
                
                # è¯„ä¼°æ—¶é—´
                pre_time = pretrained["evaluation_time"]
                fine_time = finetuned["evaluation_time"]
                print(f"{'è¯„ä¼°æ—¶é—´(ç§’)':<20} {pre_time:<15.2f} {fine_time:<15.2f} {fine_time-pre_time:+.2f}")
                
                print("\\nğŸ¯ å¾®è°ƒæ•ˆæœæ€»ç»“:")
                if exact_improvement > 0:
                    print(f"  âœ… ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡æå‡äº† {exact_improvement:.3f} ({exact_relative:+.1f}%)")
                else:
                    print(f"  âŒ ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä¸‹é™äº† {abs(exact_improvement):.3f} ({abs(exact_relative):.1f}%)")
                
                if partial_improvement > 0:
                    print(f"  âœ… éƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡æå‡äº† {partial_improvement:.3f} ({partial_relative:+.1f}%)")
                else:
                    print(f"  âŒ éƒ¨åˆ†åŒ¹é…å‡†ç¡®ç‡ä¸‹é™äº† {abs(partial_improvement):.3f} ({abs(partial_relative):.1f}%)")
                
                # æ•´ä½“è¯„ä»·
                if exact_improvement > 0 and partial_improvement > 0:
                    print("\\nğŸ‰ å¾®è°ƒæˆåŠŸï¼æ¨¡å‹åœ¨ä¸¤ä¸ªæ ¸å¿ƒæŒ‡æ ‡ä¸Šéƒ½æœ‰æå‡")
                elif exact_improvement > 0 or partial_improvement > 0:
                    print("\\nğŸˆ å¾®è°ƒéƒ¨åˆ†æˆåŠŸï¼Œåœ¨æŸäº›æŒ‡æ ‡ä¸Šæœ‰æå‡")
                else:
                    print("\\nâš ï¸ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    evaluator = Qwen3Evaluator()
    results = evaluator.run_comparison()
    return results

if __name__ == "__main__":
    main()