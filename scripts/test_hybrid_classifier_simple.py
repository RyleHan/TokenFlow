#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç®€åŒ–æµ‹è¯•ï¼šä»…æµ‹è¯•æ··åˆåˆ†ç±»å™¨æ€§èƒ½
ä¸ä¾èµ–æ–‡æ¡£æ£€ç´¢ç»„ä»¶
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.classifiers.hybrid_intent_classifier import HybridIntentClassifier
from src.classifiers.intent_classifier import MockIntentClassifier
import json
import time
from typing import Dict, List, Any

class SimpleClassifierTester:
    """ç®€åŒ–åˆ†ç±»å™¨æµ‹è¯•å™¨"""
    
    def __init__(self):
        print("ğŸ”§ åˆå§‹åŒ–åˆ†ç±»å™¨æµ‹è¯•å™¨...")
        
        # åˆå§‹åŒ–ä¸åŒçš„åˆ†ç±»å™¨
        print("ğŸ“¥ åˆå§‹åŒ–Mockåˆ†ç±»å™¨...")
        self.mock_classifier = MockIntentClassifier()
        
        print("ğŸ“¥ åˆå§‹åŒ–å¾®è°ƒæ··åˆåˆ†ç±»å™¨...")
        try:
            self.finetuned_classifier = HybridIntentClassifier(use_finetuned=True)
            self.finetuned_available = True
        except Exception as e:
            print(f"âš ï¸ å¾®è°ƒåˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.finetuned_available = False
        
        # æµ‹è¯•æ ·æœ¬
        self.test_cases = [
            {
                "prompt": "æˆ‘æƒ³æŸ¥çœ‹æˆ‘çš„è®¢å•çŠ¶æ€",
                "expected_intents": ["order_management"],
                "description": "å•æ„å›¾-è®¢å•æŸ¥è¯¢"
            },
            {
                "prompt": "ç™»å½•åæŸ¥çœ‹è®¢å•å¹¶å¤„ç†æ”¯ä»˜",
                "expected_intents": ["user_auth", "order_management", "payment"],
                "description": "å¤šæ„å›¾-ç”¨æˆ·è®¤è¯+è®¢å•+æ”¯ä»˜"
            },
            {
                "prompt": "æ”¯ä»˜æˆåŠŸåå‘é€é€šçŸ¥",
                "expected_intents": ["payment", "notification"],
                "description": "å¤šæ„å›¾-æ”¯ä»˜+é€šçŸ¥"
            },
            {
                "prompt": "åº“å­˜ä¸è¶³æ—¶éœ€è¦åŠæ—¶æé†’",
                "expected_intents": ["inventory", "notification"],
                "description": "å¤šæ„å›¾-åº“å­˜+é€šçŸ¥"
            },
            {
                "prompt": "ç”¨æˆ·æ³¨å†Œã€è®¢å•å¤„ç†å’Œæ”¯ä»˜ç¡®è®¤",
                "expected_intents": ["user_auth", "order_management", "payment"],
                "description": "å¤æ‚å¤šæ„å›¾"
            },
            {
                "prompt": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·",
                "expected_intents": ["none"],
                "description": "æ— å…³æŸ¥è¯¢"
            },
            {
                "prompt": "ä¿®æ”¹èº«ä»½è¦æ˜¯æœ‰é—®é¢˜å°±ä»˜æ¬¾å……å€¼",
                "expected_intents": ["user_auth", "payment"],
                "description": "å¤æ‚è¡¨è¾¾-èº«ä»½+æ”¯ä»˜"
            },
            {
                "prompt": "å› ä¸ºè´¦å•é—®é¢˜éœ€è¦æ”¯ä»˜å› ä¸ºæ ‡è®°çŸ­ä¿¡",
                "expected_intents": ["payment", "notification"],
                "description": "å¤æ‚è¡¨è¾¾-æ”¯ä»˜+é€šçŸ¥"
            }
        ]
    
    def calculate_accuracy(self, predicted: List[str], expected: List[str]) -> Dict[str, float]:
        """è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡"""
        predicted_set = set(predicted)
        expected_set = set(expected)
        
        # ç²¾ç¡®åŒ¹é…
        exact_match = 1.0 if predicted_set == expected_set else 0.0
        
        # éƒ¨åˆ†åŒ¹é…ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        intersection = len(predicted_set & expected_set)
        union = len(predicted_set | expected_set)
        jaccard = intersection / union if union > 0 else 0.0
        
        # F1åˆ†æ•°
        if len(predicted_set) == 0:
            precision = 0.0
        else:
            precision = intersection / len(predicted_set)
        
        if len(expected_set) == 0:
            recall = 0.0
        else:
            recall = intersection / len(expected_set)
        
        if precision + recall == 0:
            f1 = 0.0
        else:
            f1 = 2 * precision * recall / (precision + recall)
        
        return {
            "exact_match": exact_match,
            "jaccard": jaccard,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
    
    def estimate_token_savings(self, intents: List[str], prompt: str) -> Dict[str, Any]:
        """ä¼°ç®—TokenèŠ‚çœæ•ˆæœ"""
        
        # åŸºç¡€å‡è®¾
        total_docs = 20  # æ€»æ–‡æ¡£æ•°
        avg_doc_tokens = 400  # æ¯ä¸ªæ–‡æ¡£å¹³å‡tokenæ•°
        
        # æ ¹æ®æ„å›¾è¿‡æ»¤æ–‡æ¡£
        if "none" in intents:
            # æ— å…³æŸ¥è¯¢ï¼Œæ£€ç´¢è¾ƒå°‘æ–‡æ¡£
            retrieved_docs = 2
        else:
            # æœ‰ä¸šåŠ¡æ„å›¾ï¼Œæ ¹æ®æ„å›¾æ•°é‡è°ƒæ•´æ£€ç´¢æ–‡æ¡£æ•°
            base_docs = 3
            intent_bonus = len([i for i in intents if i != "none"]) * 1
            retrieved_docs = min(base_docs + intent_bonus, 8)
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        without_filtering = total_docs * avg_doc_tokens
        with_filtering = retrieved_docs * avg_doc_tokens
        
        savings = without_filtering - with_filtering
        savings_percentage = (savings / without_filtering) * 100 if without_filtering > 0 else 0
        
        return {
            "total_docs": total_docs,
            "retrieved_docs": retrieved_docs,
            "without_filtering_tokens": without_filtering,
            "with_filtering_tokens": with_filtering,
            "tokens_saved": savings,
            "savings_percentage": savings_percentage,
            "efficiency": (total_docs - retrieved_docs) / total_docs * 100
        }
    
    def run_test(self) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹åˆ†ç±»å™¨æ€§èƒ½æµ‹è¯•...")
        
        results = {
            "test_info": {
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_test_cases": len(self.test_cases),
                "finetuned_available": self.finetuned_available
            },
            "individual_results": [],
            "summary_stats": {}
        }
        
        # ç´¯è®¡ç»Ÿè®¡
        mock_stats = {"accuracy": [], "token_savings": []}
        finetuned_stats = {"accuracy": [], "token_savings": []}
        
        for i, test_case in enumerate(self.test_cases):
            print(f"\\nğŸ“‹ æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(self.test_cases)}: {test_case['description']}")
            print(f"   è¾“å…¥: {test_case['prompt']}")
            print(f"   æœŸæœ›: {test_case['expected_intents']}")
            
            test_result = {
                "test_case": test_case,
                "results": {}
            }
            
            # 1. Mockåˆ†ç±»å™¨æµ‹è¯•
            mock_intent = self.mock_classifier.predict_top_intent(test_case['prompt'])
            mock_intents = [mock_intent]
            mock_accuracy = self.calculate_accuracy(mock_intents, test_case['expected_intents'])
            mock_token_savings = self.estimate_token_savings(mock_intents, test_case['prompt'])
            
            test_result["results"]["mock"] = {
                "predicted_intents": mock_intents,
                "accuracy_metrics": mock_accuracy,
                "token_savings": mock_token_savings
            }
            
            mock_stats["accuracy"].append(mock_accuracy["f1"])
            mock_stats["token_savings"].append(mock_token_savings["tokens_saved"])
            
            print(f"   Mock:     {mock_intents} (F1: {mock_accuracy['f1']:.3f}, èŠ‚çœ: {mock_token_savings['tokens_saved']} tokens)")
            
            # 2. å¾®è°ƒæ··åˆåˆ†ç±»å™¨æµ‹è¯•
            if self.finetuned_available:
                try:
                    finetuned_result = self.finetuned_classifier.classify(test_case['prompt'])
                    finetuned_intents = finetuned_result.get("intents", [finetuned_result.get("intent", "none")])
                    finetuned_accuracy = self.calculate_accuracy(finetuned_intents, test_case['expected_intents'])
                    finetuned_token_savings = self.estimate_token_savings(finetuned_intents, test_case['prompt'])
                    
                    test_result["results"]["finetuned"] = {
                        "predicted_intents": finetuned_intents,
                        "accuracy_metrics": finetuned_accuracy,
                        "token_savings": finetuned_token_savings,
                        "full_result": finetuned_result
                    }
                    
                    finetuned_stats["accuracy"].append(finetuned_accuracy["f1"])
                    finetuned_stats["token_savings"].append(finetuned_token_savings["tokens_saved"])
                    
                    print(f"   Finetuned: {finetuned_intents} (F1: {finetuned_accuracy['f1']:.3f}, èŠ‚çœ: {finetuned_token_savings['tokens_saved']} tokens)")
                    
                except Exception as e:
                    print(f"   âŒ å¾®è°ƒåˆ†ç±»å™¨æµ‹è¯•å¤±è´¥: {e}")
                    test_result["results"]["finetuned"] = {"error": str(e)}
            
            results["individual_results"].append(test_result)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        def calc_avg(values):
            return sum(values) / len(values) if values else 0.0
        
        results["summary_stats"] = {
            "mock": {
                "avg_f1": calc_avg(mock_stats["accuracy"]),
                "avg_token_savings": calc_avg(mock_stats["token_savings"]),
                "total_token_savings": sum(mock_stats["token_savings"])
            }
        }
        
        if self.finetuned_available and finetuned_stats["accuracy"]:
            results["summary_stats"]["finetuned"] = {
                "avg_f1": calc_avg(finetuned_stats["accuracy"]),
                "avg_token_savings": calc_avg(finetuned_stats["token_savings"]),
                "total_token_savings": sum(finetuned_stats["token_savings"])
            }
            
            # è®¡ç®—æ”¹è¿›
            f1_improvement = results["summary_stats"]["finetuned"]["avg_f1"] - results["summary_stats"]["mock"]["avg_f1"]
            token_improvement = results["summary_stats"]["finetuned"]["avg_token_savings"] - results["summary_stats"]["mock"]["avg_token_savings"]
            
            results["summary_stats"]["improvement"] = {
                "f1_improvement": f1_improvement,
                "token_savings_improvement": token_improvement,
                "f1_relative_improvement": (f1_improvement / results["summary_stats"]["mock"]["avg_f1"] * 100) if results["summary_stats"]["mock"]["avg_f1"] > 0 else 0,
                "token_relative_improvement": (token_improvement / results["summary_stats"]["mock"]["avg_token_savings"] * 100) if results["summary_stats"]["mock"]["avg_token_savings"] > 0 else 0
            }
        
        return results
    
    def generate_report(self, results: Dict[str, Any]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\\n" + "="*80)
        print("ğŸ¯ å¾®è°ƒæ¨¡å‹åˆ†ç±»å™¨æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        stats = results["summary_stats"]
        
        print("\\nğŸ“Š åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”:")
        print("-"*70)
        print(f"{'åˆ†ç±»å™¨':<15} {'å¹³å‡F1åˆ†æ•°':<12} {'å¹³å‡TokenèŠ‚çœ':<15} {'æ€»TokenèŠ‚çœ':<12}")
        print("-"*70)
        
        # Mockåˆ†ç±»å™¨
        mock_stats = stats["mock"]
        print(f"{'Mockåˆ†ç±»å™¨':<15} {mock_stats['avg_f1']:<12.3f} {mock_stats['avg_token_savings']:<15.1f} {mock_stats['total_token_savings']:<12.0f}")
        
        # å¾®è°ƒåˆ†ç±»å™¨
        if "finetuned" in stats:
            finetuned_stats = stats["finetuned"]
            print(f"{'å¾®è°ƒæ··åˆ':<15} {finetuned_stats['avg_f1']:<12.3f} {finetuned_stats['avg_token_savings']:<15.1f} {finetuned_stats['total_token_savings']:<12.0f}")
            
            # æ”¹è¿›ç»Ÿè®¡
            if "improvement" in stats:
                imp = stats["improvement"]
                print("\\nğŸ’¡ å¾®è°ƒæ•ˆæœåˆ†æ:")
                print(f"  ğŸ“ˆ F1åˆ†æ•°æ”¹è¿›: {imp['f1_improvement']:+.3f} ({imp['f1_relative_improvement']:+.1f}%)")
                print(f"  ğŸ“‰ TokenèŠ‚çœæå‡: {imp['token_savings_improvement']:+.1f} tokens ({imp['token_relative_improvement']:+.1f}%)")
                
                print("\\nğŸ¯ å¾®è°ƒæ¨¡å‹è¯„ä¼°:")
                if imp['f1_improvement'] > 0 and imp['token_savings_improvement'] > 0:
                    print("  ğŸŒŸ å¾®è°ƒå–å¾—å…¨é¢æˆåŠŸï¼å‡†ç¡®ç‡å’ŒTokenèŠ‚çœéƒ½æœ‰æ˜¾è‘—æå‡")
                elif imp['f1_improvement'] > 0.1:
                    print("  âœ¨ å¾®è°ƒåœ¨å‡†ç¡®ç‡æ–¹é¢å–å¾—æ˜¾è‘—æˆåŠŸ")
                elif imp['token_savings_improvement'] > 100:
                    print("  âš¡ å¾®è°ƒåœ¨TokenèŠ‚çœæ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›")
                elif imp['f1_improvement'] > 0:
                    print("  ğŸ“ˆ å¾®è°ƒæœ‰ä¸€å®šæ•ˆæœï¼Œå‡†ç¡®ç‡æœ‰æ‰€æå‡")
                else:
                    print("  ğŸ”§ å¾®è°ƒæ•ˆæœæœ‰é™ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥")
        else:
            print("âš ï¸ å¾®è°ƒåˆ†ç±»å™¨æµ‹è¯•å¤±è´¥")
        
        # å¤šæ„å›¾è¯†åˆ«èƒ½åŠ›åˆ†æ
        print("\\nğŸª å¤šæ„å›¾è¯†åˆ«èƒ½åŠ›åˆ†æ:")
        multi_intent_cases = [case for case in results["individual_results"] if len(case["test_case"]["expected_intents"]) > 1 and case["test_case"]["expected_intents"] != ["none"]]
        
        if multi_intent_cases:
            print(f"  ğŸ“Š å¤šæ„å›¾æµ‹è¯•æ¡ˆä¾‹: {len(multi_intent_cases)}ä¸ª")
            
            if "finetuned" in stats:
                multi_intent_performance = []
                for case in multi_intent_cases:
                    if "finetuned" in case["results"] and "error" not in case["results"]["finetuned"]:
                        f1 = case["results"]["finetuned"]["accuracy_metrics"]["f1"]
                        multi_intent_performance.append(f1)
                
                if multi_intent_performance:
                    avg_multi_f1 = sum(multi_intent_performance) / len(multi_intent_performance)
                    print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹å¤šæ„å›¾å¹³å‡F1: {avg_multi_f1:.3f}")
                    
                    if avg_multi_f1 > 0.7:
                        print("  ğŸŒŸ å¤šæ„å›¾è¯†åˆ«èƒ½åŠ›ä¼˜ç§€")
                    elif avg_multi_f1 > 0.5:
                        print("  âœ… å¤šæ„å›¾è¯†åˆ«èƒ½åŠ›è‰¯å¥½")
                    elif avg_multi_f1 > 0.3:
                        print("  ğŸ“ˆ å¤šæ„å›¾è¯†åˆ«èƒ½åŠ›ä¸€èˆ¬ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
                    else:
                        print("  âš ï¸ å¤šæ„å›¾è¯†åˆ«èƒ½åŠ›éœ€è¦æ”¹è¿›")

def main():
    """ä¸»å‡½æ•°"""
    tester = SimpleClassifierTester()
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    tester.generate_report(results)
    
    # ä¿å­˜ç»“æœ
    with open("reports/simple_classifier_test.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\\nâœ… è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: reports/simple_classifier_test.json")

if __name__ == "__main__":
    main()