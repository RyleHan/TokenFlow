#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¿«é€Ÿè¯„ä¼°å¾®è°ƒåQwen3æ¨¡å‹
ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨æ ¸å¿ƒæŒ‡æ ‡
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import time
from typing import List, Dict

def load_test_samples(n=20):
    """åŠ è½½å°‘é‡æµ‹è¯•æ ·æœ¬"""
    samples = []
    with open("data/enhanced_multi_intent_training_data.jsonl", 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= n:
                break
            sample = json.loads(line.strip())
            samples.append({
                "prompt": sample["prompt"],
                "expected": sample["output"]
            })
    return samples

def quick_predict(model, tokenizer, prompt, device):
    """å¿«é€Ÿé¢„æµ‹"""
    input_text = f"ç”¨æˆ·: {prompt}\\nåŠ©æ‰‹: "
    
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=256)
    if device.type != "cpu":
        inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=20,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predicted = response.split("åŠ©æ‰‹: ")[-1].strip()
    
    # ç®€å•è§£ææ„å›¾
    intent_categories = ["order_management", "user_auth", "payment", "inventory", "notification", "none"]
    predicted_intents = []
    
    for intent in intent_categories:
        if intent in predicted.lower():
            predicted_intents.append(intent)
    
    if not predicted_intents:
        predicted_intents = ["none"]
    
    return predicted_intents

def calculate_accuracy(predictions, targets):
    """è®¡ç®—å‡†ç¡®ç‡"""
    exact_match = 0
    partial_match = 0
    total = len(predictions)
    
    for pred, target in zip(predictions, targets):
        pred_set = set(pred)
        target_set = set(target)
        
        if pred_set == target_set:
            exact_match += 1
            partial_match += 1
        elif pred_set & target_set:
            partial_match += 1
    
    return {
        "exact_match": exact_match / total,
        "partial_match": partial_match / total,
        "total": total
    }

def main():
    print("ğŸ¯ Qwen3-0.6Bå¾®è°ƒæ•ˆæœå¿«é€ŸéªŒè¯")
    print("=" * 50)
    
    # è®¾å¤‡æ£€æŸ¥
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… ä½¿ç”¨MPSåŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("âš ï¸ ä½¿ç”¨CPU")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_samples = load_test_samples(20)
    print(f"åŠ è½½æµ‹è¯•æ ·æœ¬: {len(test_samples)}ä¸ª")
    
    results = {}
    
    try:
        # 1. æµ‹è¯•å¾®è°ƒåæ¨¡å‹
        print("\\nğŸ” æµ‹è¯•å¾®è°ƒåæ¨¡å‹...")
        finetuned_tokenizer = AutoTokenizer.from_pretrained(
            "models/qwen3_fixed_classifier", trust_remote_code=True
        )
        if finetuned_tokenizer.pad_token is None:
            finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-0.6B", torch_dtype=torch.float32, trust_remote_code=True
        )
        finetuned_model = PeftModel.from_pretrained(base_model, "models/qwen3_fixed_classifier")
        finetuned_model = finetuned_model.to(device)
        
        finetuned_predictions = []
        start_time = time.time()
        
        for sample in test_samples:
            pred = quick_predict(finetuned_model, finetuned_tokenizer, sample["prompt"], device)
            finetuned_predictions.append(pred)
        
        finetuned_time = time.time() - start_time
        
        # è®¡ç®—å¾®è°ƒåå‡†ç¡®ç‡
        targets = [sample["expected"] for sample in test_samples]
        finetuned_accuracy = calculate_accuracy(finetuned_predictions, targets)
        
        results["finetuned"] = {
            "exact_match_accuracy": finetuned_accuracy["exact_match"],
            "partial_match_accuracy": finetuned_accuracy["partial_match"],
            "evaluation_time": finetuned_time,
            "samples": finetuned_accuracy["total"]
        }
        
        print(f"å¾®è°ƒåæ¨¡å‹:")
        print(f"  ç²¾ç¡®åŒ¹é…: {finetuned_accuracy['exact_match']:.3f}")
        print(f"  éƒ¨åˆ†åŒ¹é…: {finetuned_accuracy['partial_match']:.3f}")
        print(f"  è¯„ä¼°è€—æ—¶: {finetuned_time:.2f}ç§’")
        
        # æ˜¾ç¤ºéƒ¨åˆ†é¢„æµ‹ç»“æœ
        print("\\nğŸ“‹ é¢„æµ‹ç¤ºä¾‹:")
        for i in range(min(5, len(test_samples))):
            sample = test_samples[i]
            pred = finetuned_predictions[i]
            target = sample["expected"]
            match = "âœ…" if set(pred) == set(target) else ("ğŸŸ¡" if set(pred) & set(target) else "âŒ")
            
            print(f"  {match} è¾“å…¥: {sample['prompt'][:30]}...")
            print(f"     æœŸæœ›: {target}")
            print(f"     é¢„æµ‹: {pred}")
        
        del finetuned_model, base_model
        if device.type == "mps":
            torch.mps.empty_cache()
        
    except Exception as e:
        print(f"âŒ å¾®è°ƒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        results["finetuned"] = {"error": str(e)}
    
    # 2. å¯¹æ¯”ä¹‹å‰çš„åŸºçº¿ç»“æœ
    print("\\nğŸ“Š ä¸åŸºçº¿å¯¹æ¯”:")
    baseline_path = "reports/pretrained_qwen3_test.json"
    try:
        with open(baseline_path, 'r', encoding='utf-8') as f:
            baseline_data = json.load(f)
        
        pretrained_accuracy = baseline_data["results"]["pretrained_qwen3"]["overall_accuracy"]
        mock_accuracy = baseline_data["results"]["mock"]["overall_accuracy"]
        
        print(f"é¢„è®­ç»ƒæ¨¡å‹å‡†ç¡®ç‡: {pretrained_accuracy:.3f}")
        print(f"Mockåˆ†ç±»å™¨å‡†ç¡®ç‡: {mock_accuracy:.3f}")
        
        if "finetuned" in results and "error" not in results["finetuned"]:
            finetuned_acc = results["finetuned"]["exact_match_accuracy"]
            improvement = finetuned_acc - pretrained_accuracy
            relative_improvement = (improvement / pretrained_accuracy * 100) if pretrained_accuracy > 0 else 0
            
            print(f"å¾®è°ƒåæ¨¡å‹å‡†ç¡®ç‡: {finetuned_acc:.3f}")
            print(f"\\nğŸ¯ å¾®è°ƒæ•ˆæœ:")
            print(f"  ç»å¯¹æå‡: +{improvement:.3f}")
            print(f"  ç›¸å¯¹æå‡: +{relative_improvement:.1f}%")
            
            gap_before = mock_accuracy - pretrained_accuracy
            gap_after = mock_accuracy - finetuned_acc
            print(f"  ä¸Mockå·®è·: {gap_before:.3f} â†’ {gap_after:.3f}")
            
            if finetuned_acc > pretrained_accuracy:
                print("\\nğŸ‰ å¾®è°ƒæˆåŠŸï¼æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡")
            else:
                print("\\nâš ï¸ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        
        results["baseline_comparison"] = {
            "pretrained_accuracy": pretrained_accuracy,
            "mock_accuracy": mock_accuracy,
            "improvement": improvement if "finetuned" in results else None
        }
        
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½åŸºçº¿ç»“æœ: {e}")
    
    # ä¿å­˜ç»“æœ
    results["test_info"] = {
        "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "test_samples": len(test_samples),
        "device": str(device)
    }
    
    with open("reports/quick_finetuned_evaluation.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\\nâœ… å¿«é€Ÿè¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: reports/quick_finetuned_evaluation.json")
    return results

if __name__ == "__main__":
    main()